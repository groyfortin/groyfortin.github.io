<HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html;
charset=iso-8859-1">

<style>
h1 {
    color:black;
    font-family:helvetica;
    font-size:300%;
}

h3 {
    color:black;
    font-family:helvetica;
    font-variant: small-caps;
    font-size:120%;
}
p  {
    color:black;
    font-family:helvetica;
    font-size:90%
}

sm  {
    color:black;
    font-family:helvetica;
    font-size:70%
}
</style>
<TITLE>Séminaire sur les mathématiques de l'apprentissage machine</TITLE>


Ce séminaire est consacré à l'exploration de différents thèmes mathématiques reliés à l'apprentissage machine, notamment au deep learning. 
<br>
C'est un séminaire <i> mathématique </i>: il existe déjà des cours sur l'apprentissage machine à l'ÉTS! <br> L'objectif complémentaire de ce séminaire est donc d'explorer
l'aspect mathématique de ce domaine effervescent. <br> 
<br> 
Il s'agit d'un séminaire informel et exploratoire qui est ouvert à tous. Une certaine connaissance du calcul différentiel et de l'algèbre linéaire est préférable <br>
pour suivre la plupart des exposés. Le séminaire est tenu chaque deux semaines, consulter l'horaire plus bas pour les détails.  

<br>
<br>

<h2> Automne 2019 </H2> 
<hr> 
<ul>
<li type="square"><p><h3>Introduction aux mathématiques des réseaux de neurones</H3>
<i> 18 septembre 2019, de 13h30 à 15h. B-1508 </i>

<br> 
<br> 
Introduction aux réseaux de neurones. Poids, biais, fonction d'activation, fonction d'erreur. <br> Point de vue matriciel: vecteurs et transformations affines.
<br> Optimisation, descente stochastique de gradient. <br> Dérivation des équations de la propagation arrière.
</p></li>
<p><big>→</big> La <A HREF="https://youtu.be/kzH69dnPjX4"><b>vidéo</b></A> et les <A HREF="http://groyfortin.github.io/slides_dl_sem1.pdf"><b>slides</b></A> de la présentation.</p> 

<small><i> Note: Malheureusement, les diapositives n'apparaîssent pas dans la vidéo, ce pourquoi il est suggéré aux intéressés de les consulter en parallèle au visionnement.</i></small>



<li type="square"><p><h3>Universalité des réseaux de neurones: théorèmes de Cybenko et Hornik</H3>
<i> 2 octobre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>
Universalité: théorèmes de Cybenko, Hornik.<br>
Théorèmes de Hahn-Banach, représentation de Riesz.<br>
Éléments de théorie de la mesure, théorèmes de convergence.<br>
Preuve du théorème de Cybenko pour les sigmoïdes sur les espaces de fonctions continues.<br>


</p></li>

<li type="square"><p><h3>Le rôle de la profondeur dans la puissance expressive des réseaux de neurones</H3>
<i> 16 octobre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>
Universalité pour les réseaux de neurones<br>
Puissance expressive: profondeur vs. largeur.<br>
Théorèmes de Telgarsky, Eldan-Shamir.<br>

</p></li>


<li type="square"><p><h3>Réduction de la dimensionalité: analyse en composantes principales</H3> 
<i> 30 octobre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>
Rappels d'algèbre linéaire: normes L2 et de Fröbenius, éléments de théorie spectrale.<br> Dérivation des équations normales. 

</p></li>
<li type="square"><p><h3>Réduction de dimensionalité via le laplacien</H3>
<i> 13 novembre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>
Théorèmes de Belkin et de Belkin-Niyogi. 
</p></li>


<li type="square"><p><h3>Gradients problématiques et distribution statistique des paramètres: théorèmes de Hanin.</H3>
<i> 27 novembre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>

</p></li>


<li type="square"><p><h3>Régularisation et optimisation des réseaux de neurones</H3>
<i> 11 décembre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br> 
Fonctions d'activations: sigmoïdales, entropie croisée et softmax. 
<br> Overfitting et techniques de régularisation: diminution des poids, L2 et L1, dropout. <br>
Descente hessienne, descente de gradient dynamique.
</p></li>

<br>
<hr>

<p><A HREF="http://groyfortin.github.io"> Retour à la page d'accueil. </A></p> 
<br> 

<ul>
</BODY>