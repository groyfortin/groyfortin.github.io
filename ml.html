<HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html;
charset=iso-8859-1">

<style>
h1 {
    color:black;
    font-family:helvetica;
    font-size:300%;
}

h3 {
    color:black;
    font-family:helvetica;
    font-variant: small-caps;
    font-size:120%;
}
p  {
    color:black;
    font-family:helvetica;
    font-size:90%
}

sm  {
    color:black;
    font-family:helvetica;
    font-size:70%
}
</style>
<TITLE>Séminaire sur les mathématiques de l'apprentissage machine</TITLE>


Ce séminaire est consacré à l'étude de différents thèmes mathématiques reliés à l'apprentissage machine, notamment au deep learning. 
<br>
C'est un séminaire exploratoire ouvert à tous. Une certaine connaissance du calcul différentiel et de l'algèbre linéaire est préférable <br>
 pour suivre la plupart des exposés. Le séminaire est tenu chaque deux semaines, consulter l'horaire plus bas pour les détails.  

<br>
<br>

<h2> Automne 2019 </H2> 
<hr> 
<ul>
<li type="square"><p><h3>Introduction aux mathématiques des réseaux de neurones</H3>
<i> 18 septembre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br> 
Introduction aux réseaux de neurones. Poids, biais, fonction d'activation, fonction d'erreur. <br> Point de vue matriciel: vecteurs et transformations affines.
<br> Optimisation, descente stochastique de gradient. <br> Dérivation des équations de la propagation arrière.
</p></li>


<li type="square"><p><h3>Régularisation et optimisation des réseaux de neurones</H3>
<i> 2 octobre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br> 
Fonctions d'activations: sigmoïdales, entropie croisée et softmax. 
<br> Overfitting et techniques de régularisation: diminution des poids, L2 et L1, dropout. <br>
Descente hessienne, descente de gradient dynamique.
</p></li>

<li type="square"><p><h3>Algèbre linéaire et analyse en composantes principales</H3> 
<i> 16 octobre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>


</p></li>
<li type="square"><p><h3>Réduction de dimensionalité via le laplacien: théorèmes de Belkin</H3>
<i> 30 octobre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>



</p></li>
<li type="square"><p><h3>Universalité des réseaux de neurones: théorèmes de Cybenko et Hornik</H3>
<i> 13 novembre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>



</p></li>
<li type="square"><p><h3>Le rôle de la profondeur dans la puissance expressive des réseaux de neurone</H3>
<i> 27 novembre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>


</p></li>
<li type="square"><p><h3>Gradients problématiques et distribution statistique des paramètres: théorèmes de Hanin.</H3>
<i> 11 décembre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br>

</p></li>


<ul>
</BODY>