<HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html;
charset=iso-8859-1">

<style>
h1 {
    color:black;
    font-family:helvetica;
    font-size:300%;
}

h3 {
    color:black;
    font-family:helvetica;
    font-variant: small-caps;
    font-size:120%;
}
p  {
    color:black;
    font-family:helvetica;
    font-size:90%
}

sm  {
    color:black;
    font-family:helvetica;
    font-size:70%
}
</style>
<TITLE>Séminaire sur les mathématiques de l'apprentissage machine</TITLE>


Ce séminaire est consacré à l'exploration de différents thèmes mathématiques reliés à l'apprentissage machine, notamment au deep learning. 
<br>
C'est un séminaire <i> mathématique </i>: il existe déjà des cours sur l'apprentissage machine à l'ÉTS! <br> L'objectif complémentaire de ce séminaire est donc d'explorer
l'aspect mathématique de ce domaine effervescent. <br> 
<br> 
Il s'agit d'un séminaire informel et exploratoire qui est ouvert à tous. Une certaine connaissance du calcul différentiel et de l'algèbre linéaire est préférable <br>
pour suivre la plupart des exposés. Le séminaire est tenu chaque deux semaines, consulter l'horaire plus bas pour les détails.  

<br>
<br>

<h2> Automne 2019 </H2> 
<hr> 
<ul>
<li type="square"><p><h3>Introduction aux mathématiques des réseaux de neurones</H3>
<i> 18 septembre 2019, de 13h30 à 15h. B-1508 </i>

<br> 
<br> 
Introduction aux réseaux de neurones. Poids, biais, fonction d'activation, fonction d'erreur. <br> Point de vue matriciel: vecteurs et transformations affines.
<br> Optimisation, descente stochastique de gradient. <br> Dérivation des équations de la propagation arrière.
</p></li>
<p><big>→</big> La <A HREF="http://youtu.be/kzH69dnPjX4"><b>vidéo</b></A> et les <A HREF="http://groyfortin.github.io/slides_dl_sem1.pdf"><b>slides</b></A> de la présentation.</p> 

<small><i> Note: Malheureusement, les diapositives n'apparaîssent pas dans la vidéo, ce pourquoi il est suggéré aux intéressés de les consulter en parallèle au visionnement.</i></small>



<li type="square"><p><h3>Universalité des réseaux de neurones: théorèmes de Cybenko et Hornik</H3>
<i> 2 octobre 2019, de 13h30 à 15h. B-1510 </i>
<br> 
<br>
Universalité: théorèmes de Cybenko, Hornik<br>
Théorèmes de Hahn-Banach, représentation de Riesz.<br>
Éléments de théorie de la mesure.<br>
Preuve du théorème de Cybenko pour les sigmoïdes sur les espaces de fonctions continues.<br>
</p></li>
<p><big>→</big> La <A HREF="http://youtu.be/fKltoS2pvGg"><b>vidéo</b></A>.<br>
 <big>→</big> Une <A HREF="http://groyfortin.github.io/preuve_sigmoides.pdf"><b>preuve</b></A> que les fonctions sigmoïdales bornées sont discriminatoires. </p>


<li type="square"><p><h3>Le rôle de la profondeur dans la puissance expressive des réseaux de neurones</H3>
<i> 16 octobre 2019, de 13h30 à 15h. B-3420. </i>
<br> 
<br>
Retour sur l'universalité pour les réseaux de neurones.<br>
Heuristique de la densité des réseaux de neurones dans l'espace des fonctions bornées. <br>
Puissance expressive: profondeur vs. largeur.<br>
Théorèmes de Telgarsky, Eldan-Shamir.<br>
</p></li>
<p><big>→</big> La <A HREF="http://youtu.be/18W_MvHpctg"><b>vidéo</b></A>.<br>

</p></li>


<li type="square"><p><h3>Introduction à l'apprentissage profond géométrique</H3>
<i> 6 novembre 2019, de 13h30 à 15h. B-1510. </i>
<br> 
<br>
Classification d'images et réseaux de convolution.<br>
Préliminaires: algèbre linéaire, théorie de Fourier, convolution et théorie spectrale du laplacien.<br>
Théorie spectrale sur les graphes non-orientés.<br>
SGCNN: Spectral Graph Convolutional Networks.<br>
</p></li>

<p><big>→</big> La <A HREF="http://youtu.be/OFnSjoKzYyg"><b>vidéo</b></A>.<br>

</p></li>


<li type="square"><p><h3>Réduction spectrale de la dimensionalité</H3> 
<i> 20 novembre 2019, de 13h30 à 15h. B-1510. </i>
<br> 
<br>
Analyse en composantes principales.<br>
Variétés riemanniennes et plongements euclidiens.<br>
Caractérisation variationnelle des valeurs propres d'opérateurs symétriques. <br>
Algorithmes de Belkin-Niyogi pour les plongements de graphe.<br>
</p></li>



<li type="square"><p><h3>Réduction de dimensionalité via le laplacien</H3>
<i> 4 décembre 2019, de 13h30 à 15h. B-1510. </i>
<br> 
<br>
Théorèmes de Belkin et de Belkin-Niyogi. 
</p></li>


<li type="square"><p><h3>Gradients problématiques et distribution statistique des paramètres: théorèmes de Hanin.</H3>
<i> 18 décembre 2019, de 13h30 à 15h. B-1510. </i>
<br> 
<br>

</p></li>


<br>
<br>

<h2> Hiver 2020 </H2> 
<hr> 
<ul>


<li type="square"><p><h3>Régularisation et optimisation des réseaux de neurones</H3>
<i> Janvier 2020, de 13h30 à 15h. Local à déterminer. </i>
<br> 
<br> 
Fonctions d'activations: sigmoïdales, entropie croisée et softmax. 
<br> Overfitting et techniques de régularisation: diminution des poids, L2 et L1, dropout. <br>
Descente hessienne, descente de gradient dynamique.
</p></li>

<br>
<hr>

<p><A HREF="http://groyfortin.github.io"> Retour à la page d'accueil. </A></p> 
<br> 

<ul>
</BODY>