<HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html;
charset=iso-8859-1">

<style>
h1 {
    color:black;
    font-family:helvetica;
    font-size:300%;
}

h3 {
    color:black;
    font-family:helvetica;
    font-variant: small-caps;
    font-size:120%;
}
p  {
    color:black;
    font-family:helvetica;
    font-size:90%
}

sm  {
    color:black;
    font-family:helvetica;
    font-size:70%
}
</style>
<TITLE>Séminaire sur les mathématiques de l'apprentissage machine</TITLE>


Ce séminaire est consacré à l'étude de différents thèmes mathématiques reliés à l'apprentissage machine, notamment au deep learning. 
<br>
C'est un séminaire exploratoire ouvert à tous. Une certaine connaissance du calcul différentielle et de l'algèbre linéaire est préférable pour 
suivre la plupart des exposés. <br> Le séminaire est tenu chaque deux semaines, consulter l'horaire plus bas pour les détails.  

<br>
<hr> 
<br>

<h2> Automne 2019 </H2> 

<br> 
<hr> 

<ul>
<li type="square"><p><h3>Introduction aux mathématiques des réseaux de neurones</H3>
<br>
<i> 18 septembre 2019, de 13h30 à 15h. Local à déterminer. </i>
<br> 
Introduction aux réseaux de neurones. Poids, biais, fonction d'activation, fonction d'erreur. <br> Point de vue matriciel: vecteurs et transformations affines.
<br> Optimisation, descente stochastique de gradient. <br> Dérivation des équations de la propagation arrière.
</p></li>


<li type="square"><p>Régularisation et optimisation des réseaux de neurones</p></li>
<li type="square"><p>Algèbre linéaire et analyse en composantes principales</p></li>
<li type="square"><p>Réduction de dimensionalité via le laplacien: théorèmes de Belkin</p></li>
<li type="square"><p>Universalité des réseaux de neurones: théorèmes de Cybenko et Hornik</p></li>
<li type="square"><p>Le rôle de la profondeur dans la puissance expressive des réseaux de neurone</p></li>
<li type="square"><p>Gradients problématiques et distribution statistique des paramètres</p></li>


<ul>
</BODY>